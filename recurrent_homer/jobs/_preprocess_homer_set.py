from pathlib import Path

import pandas as pd

from recurrent_homer.constants import DATA_PATH
from recurrent_homer.model.text_vectorizer import TextVectorizer

from .utils import create_tf_dataset, shuffle_dataset, train_val_test_split


def preprocess_homer_dataset(
    text_vectorizer: TextVectorizer, val_test_proportions: tuple, batch_size: int
) -> tuple:
    """Preprocess data to train the model, the following steps are executed:
    1. Extract homer text from kaggle dataset.
    3. Generate TensorFlow dataset with ids generated by `text_vectorizer` provided.

    Args:
    text_vectorizer (TextVectorizer): Text vectorizer initialized.
    val_test_proportions (tuple): Percentage of the total set to ve used as
      validation and test set. Fore example, for (10, 10), 10% of the dataset
      will be used for validation, 10% for testing and the remaining 80% for train
    batch_size (int): Batch size to use.

    Returns:
    tuple: Train, validation and test sets.
    """
    simpson_scripts = pd.read_csv(DATA_PATH / "simpsons_script_lines.csv")
    homer_text = _extract_homer_text(simpson_scripts)

    full_dataset = create_tf_dataset(homer_text, text_vectorizer)
    full_shuffled = shuffle_dataset(full_dataset, batch_size)
    train, validation, test = train_val_test_split(full_shuffled, val_test_proportions)

    return train, validation, test


def _extract_homer_text(script_frame: pd.DataFrame) -> str:
    """Extract Homer simpson's phrases from dataset and merge them
    together in a unique corpus.

    Args:
        script_frame (pd.DataFrame): Dataset frame with Simpsons scripts.

    Returns:
        str: Homer text corpus extracted from dataset.
    """
    homer_phrases = script_frame.loc[
        lambda x: (x["raw_character_text"] == "Homer Simpson") & ~(x["normalized_text"].isna())
    ]
    homer_phrases = homer_phrases.astype({"normalized_text": "str"})
    return "\n ".join(homer_phrases["normalized_text"])
