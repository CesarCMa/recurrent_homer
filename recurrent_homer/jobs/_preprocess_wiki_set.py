"""`preprocess_wiki_set` module."""

import logging

import tensorflow as tf
from datasets import load_dataset

from recurrent_homer.model.text_vectorizer import TextVectorizer

from .utils import create_tf_dataset, shuffle_dataset, train_val_test_split

logger = logging.getLogger(__name__)


def preprocess_wiki_dataset(
    val_test_proportions: tuple, amount_samples: int, batch_size: int
) -> tuple:
    """Preprocess data to train the model, the following steps are executed:
    1. Extract `amount_samples` samples from `wikipedia` dataset using huggingface `datasets` module.
    2. Create an Instance of a Text Vextorizer with extracted text.
    3. Generate TensorFlow dataset with ids generated by `TextVectorizer`.

    Args:
        val_test_proportions (tuple): Percentage of the total set to ve used as
          validation and test set. Fore example, for (10, 10), 10% of the dataset
          will be used for validation, 10% for testing and the remaining 80% for train.
        amount_samples (int): Number of samples to extract.
        batch_size (int): Batch size to use.

    Returns:
        tuple: Train, validation and test sets.
    """
    logger.info("Loading dataset form Huggingface datasets...")
    wiki_stream_set = load_dataset("wikipedia", "20220301.en", split="train", streaming=True)

    logger.info("Extracting text from dataset...")
    text = _extract_text(wiki_stream_set, amount_samples)

    logger.info("Creating text vectorizer...")
    text_vectorizer = TextVectorizer(text)

    logger.info("Creating TensorFlow dataset...")
    full_dataset = create_tf_dataset(text, text_vectorizer)
    full_shuffled = shuffle_dataset(full_dataset, batch_size)
    train, validation, test = train_val_test_split(full_shuffled, val_test_proportions)

    return train, validation, test, text_vectorizer


def _extract_text(stream_set, amount_samples: int) -> str:
    """Extract text from dataset.

    Args:
        stream_set: Dataset to extract text from.
        amount_samples (int): Number of samples to extract.

    Returns:
        str: Text extracted from dataset.
    """
    full_text = ""
    for count, sample in enumerate(stream_set.take(amount_samples)):
        logger.info(f"Processing sample {count}")
        full_text = full_text + "\n" + sample["text"]
    return full_text
